{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the data cleaning phase of this project!\n",
    "We currently have a folder `table-data` with a bunch of dirty json files. The goal of this notebook is to illustrate the process of data cleaning by subsequently transforming the dirty json tables into dataframes with tidied up data, ready to be stored in a database and then analyzed!\n",
    "\n",
    "There are 6 different tables for every printing log:\n",
    "* Overview\n",
    "* Bioinks\n",
    "* Material Settings\n",
    "* Printer Setup\n",
    "* Hardware Setup\n",
    "* Printing Log\n",
    "\n",
    "The cleaning steps will refer to the codes B1 - B5 and represent the listed functions stated in the theoretical part of the thesis. B1 was already achieved in the file `structureTables.js`. The starting point of this notebook is a folder containing structured json files with added `id` field referencing the corresponding experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load mappings from JSON file\n",
    "with open('mappings.json', 'r') as file:\n",
    "    mappings = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function which is used throughout this notebook to access the mapping-dictionary in `mappings.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find the key for a given value\n",
    "def find_key(mapping, value):\n",
    "    value = value.strip().lower()  # Strip whitespaces and convert to lower case for comparison\n",
    "    for key, values in mapping.items():\n",
    "        if any(val.lower() in value for val in values):  # Compare lower case values\n",
    "            return key\n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframes from Json (B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate through each JSON file within the 'overview' category, aggregating their contents into a single comprehensive dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, \"overview\" + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "overviewDf = pd.DataFrame(data)\n",
    "\n",
    "overviewDf.to_csv(\"data-frames-raw/\" + \"overview\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bioinks Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, \"bioInks\" + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "bioInksDf = pd.DataFrame(data)\n",
    "bioInksDf.to_csv(\"data-frames-raw/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Setup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, \"hardwareSetup\" + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "hardwareSetupDf = pd.DataFrame(data)\n",
    "\n",
    "hardwareSetupDf.to_csv(\"data-frames-raw/\" + \"hardwareSetup\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printer Setup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, \"printerSetup\" + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "printerSetupDf = pd.DataFrame(data)\n",
    "\n",
    "printerSetupDf.to_csv(\"data-frames-raw/\" + \"printerSetup\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printerSetup table only contains filenames and has a lot of missing values. Thus, it is considered irrelevant for this analysis and can be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material Settings Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, \"materialSettings\" + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "materialSettingDf = pd.DataFrame(data)\n",
    "\n",
    "materialSettingDf.to_csv(\"data-frames-raw/\" + \"materialSettings\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Log Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, \"printingLog\" + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "printingLogDf = pd.DataFrame(data)\n",
    "\n",
    "printingLogDf.to_csv(\"data-frames-raw/\" + \"printingLog\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Columns (B3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table doesn't have unnecessary columns but the column 'folderName' contains date information so a new column 'date' is needed. Furthermore, the column names should be converted to lowercase.\n",
    "Also, the entries in the column 'Log no.' seem to be subset of the entries in the column 'folderName' and thus can be dropped to reduce redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns\n",
    "overviewDf['date'] = None\n",
    "\n",
    "# Drop column 'Log no.'\n",
    "overviewDf = overviewDf.drop(columns=['Log no.'])\n",
    "\n",
    "# Convert column names to lowercase\n",
    "overviewDf.columns = overviewDf.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewDf.to_csv(\"data-frames-cleaned/\" + \"overview\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bioinks Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has several columns which are mostly empty. Before removing columns, let's look at them, maybe we can map some to similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name, Polymer, cpolymer [v%], cLAP [wt%], CTartrazine [mM], Solvent, logId, spheres, CIodixanol [v/v], Gelma FIC A29, other, cLAP [v%], CDTT [v%], Color [mg], additives, Additives, Filter [µM], Add, Fluorospheres [dil], Comment, Amount of color(mg/ml), Beads Mio/ml, cDTT [v%], Peptide (g/L)\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(bioInksDf.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cLAP [v%] and cLAP [wt%] describe the same thing but are in different units. Let's change that. Same goes for CDTT [v%] and CTartrazine [mM]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "bioInksDf['CTartrazine [mM]'] = bioInksDf['CTartrazine [mM]'].combine_first(bioInksDf['CDTT [v%]'])\n",
    "# only get the values in mM\n",
    "\n",
    "def extract_mM(value):\n",
    "    # Check if value is a string\n",
    "    if isinstance(value, str):\n",
    "        # Use regular expression to find the pattern\n",
    "        match = re.search(r'\\((\\d+)mM\\)', value)\n",
    "        if match:\n",
    "            # Extract and return the number\n",
    "            return int(match.group(1))\n",
    "    # Return NaN or some default value if pattern not found or value is not a string\n",
    "    return value\n",
    "\n",
    "# Apply the function to the desired column\n",
    "bioInksDf['CTartrazine [mM]'] = bioInksDf['CTartrazine [mM]'].apply(extract_mM)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step we will remove the superfluous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns ['spheres', 'CIodixanol [v/v]', 'Gelma FIC A29', 'other', 'cLAP [v%]', 'CDTT [v%]', 'Color [mg]', 'additives', 'Additives', 'Filter [µM]', 'Add', 'Fluorospheres [dil]', 'Comment', 'Amount of color(mg/ml)', 'Beads Mio/ml', 'cDTT [v%]', 'Peptide (g/L)'] removed for type <class 'type'>.\n"
     ]
    }
   ],
   "source": [
    "rmCols = [\n",
    "    \"spheres\" , \"CIodixanol [v/v]\", \"Gelma FIC A29\", \"other\", \"cLAP [v%]\", \"CDTT [v%]\", \"Color [mg]\", \"additives\", \"Additives\", \"Filter [µM]\", \n",
    "    \"Add\", \"Fluorospheres [dil]\", \"Comment\", \"Amount of color(mg/ml)\", \"Beads Mio/ml\", \"cDTT [v%]\", \"Peptide (g/L)\"\n",
    "]\n",
    "\n",
    "# Remove the specified columns from the DataFrame\n",
    "bioInksDf = bioInksDf.drop(columns=rmCols, errors='ignore')\n",
    "print(f\"Columns {rmCols} removed for type {type}.\")\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names need to be converted to lower case and stripped of the white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to lowercase\n",
    "bioInksDf.columns = [col.lower() for col in bioInksDf.columns]\n",
    "\n",
    "bioInksDf.columns = bioInksDf.columns.str.replace(' ', '_').str.replace('[\\[\\]]', '', regex=True).str.replace('%', '')\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardware table has some columns which need to mapped.\n",
    "There are three columns which describe the ink reservoir/drying setup. Also, the last column 'Printhead: CB5' can be dropped since it doesn't contain values. After that, convert the column names to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_concat = ['Inkreservoir/Drying', 'Ink reservoir/drying', 'Ink reservoir/Drying']\n",
    "\n",
    "hardwareSetupDf['inkreservoir/drying'] = hardwareSetupDf.apply(\n",
    "    lambda row: row['Inkreservoir/Drying'] if pd.notnull(row['Inkreservoir/Drying'])\n",
    "    else (row['Ink reservoir/drying'] if pd.notnull(row['Ink reservoir/drying'])\n",
    "    else row['Ink reservoir/Drying']), axis=1)\n",
    "\n",
    "hardwareSetupDf = hardwareSetupDf.drop(['Inkreservoir/Drying', 'Ink reservoir/drying', 'Ink reservoir/Drying', 'Printhead: CB5'], axis=1)\n",
    "\n",
    "# Convert column names to lowercase\n",
    "hardwareSetupDf.columns = hardwareSetupDf.columns.str.lower()\n",
    "\n",
    "# Write to csv\n",
    "hardwareSetupDf.to_csv(\"data-frames-cleaned/\" + \"hardwareSetup\" + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Settings Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, merge the column .JSON with column \"Material parameters\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "materialSettingDf.loc[materialSettingDf['Material parameters'].isna(), 'Material parameters'] = materialSettingDf['.JSON']\n",
    "\n",
    "materialSettingDf = materialSettingDf.drop(['.JSON', 'Attempt', 'Material'], axis=1)\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + \"materialSettings\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"Material parameters\" has json values which need their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "columns_to_initialize = [\n",
    "    'temperature', 'temperatureTolerance', 'brightness', 'exposureTime', 'zHop',\n",
    "    'zHopSpeed', 'washTime', 'dabCount', 'washCount', 'projectionDelay'\n",
    "]\n",
    "for column in columns_to_initialize:\n",
    "    materialSettingDf[column] = \"\"\n",
    "\n",
    "def processJson(row):\n",
    "    string = row['Material parameters']\n",
    "    # Check for substrings that should cause the function to abort and return the string as is\n",
    "    if \"http\" in string or \"Slice\" in string:\n",
    "        return row\n",
    "    # Normalize the string by removing any leading/trailing braces and whitespace\n",
    "    if \"{\" in string or '\"' in string:   \n",
    "        normalized_str = re.sub(r'^\\s*{\\s*|\\s*}\\s*$', '', string)\n",
    "        pairs = re.findall(r'(?:\"(\\w+)\":\\s*([^,}]+))', normalized_str)\n",
    "    else:\n",
    "        normalized_str = string\n",
    "        pairs = re.findall(r'(?:(\\w+):\\s*([^,}]+))', normalized_str)\n",
    "\n",
    "    # Process each pair and convert numerical values from string to their appropriate type\n",
    "    for key, value in pairs:\n",
    "        # Remove any non-numeric trailing characters from the value\n",
    "        value = re.sub(r'[^0-9.]+$', '', value.strip())\n",
    "        # Try converting to integer, if fail, then to float, if fail, keep as string\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except ValueError:\n",
    "                pass  # keep the value as string if it's neither int nor float\n",
    "        # Assign the value to the row if the key is a column in the DataFrame\n",
    "        if key in materialSettingDf.columns:\n",
    "            row[key] = value\n",
    "            \n",
    "    return row\n",
    "\n",
    "materialSettingDf = materialSettingDf.apply(processJson, axis=1)\n",
    "\n",
    "# Drop useless columns\n",
    "materialSettingDf = materialSettingDf.drop([\"Material parameters\", \"File name\", \"File name of material .json\", \"File name of material .Json\"], axis=1)\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + \"materialSettings\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert columns to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to lowercase\n",
    "materialSettingDf.columns = materialSettingDf.columns.str.lower()\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + \"materialSettings\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Log Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the columns and figuring out which columns describe the same values and which columns can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Attempt No.', 'Ink', 'layer height [µm]', 'no. bottom layers', 'texp.bottom\\xa0 [s]', 'texposure_pos1 [s]', 'texposure_pos2 [s]', 'texposure_pos3 [s]', 'texposure_pos4 [s]', 'zSpeed', 'status', 'comment', 'next steps', 'pictures', 'logId', 'texposure_sec[s]', 'texposure_last [s]', 'Washing/drying process', 'Ink1', 'Ink2', 'washing', 'drying', 'WASH/ DRY technique', 'GLOBAL OFFSET', 'texp.bottom\\u202f [s]', 'Texposure_pos1 [s]', 'Texposure_pos2 [s]', 'Texposure_pos3 [s]', 'Texposure_pos4 [s]', 'texp.layer [s]', 'zHop [mm]', 'LAP [%]', 'texp.layer 1 [s]', 'texp.layer 2 [s]', 'texp.layer 3 [s]', 'texp.layer 4 [s]', 'Post-curing time [min]', 'Tart', 'layer height [mm]', 'Base height', 'OFFSET', 'VARIABLE CHANGED', 'texposure [s]', 'Tart [mM]', 'zHop', 'Comment\\xa010 s nachbelichtet']\n"
     ]
    }
   ],
   "source": [
    "print(printingLogDf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "printingLogDf = printingLogDf.drop(columns=['OFFSET', 'Tart', 'Tart [mM]', 'Base height', 'zHop', 'Post-curing time [min]', 'VARIABLE CHANGED', \n",
    "                                            'Comment 10 s nachbelichtet', 'LAP [%]', 'GLOBAL OFFSET', 'texposure [s]', 'Ink1', 'Ink2', \n",
    "                                            'Washing/drying process', 'washing', 'drying', 'WASH/ DRY technique', 'texposure_sec[s]', \n",
    "                                            'texposure_last [s]', 'texp.bottom  [s]', 'Texposure_pos2 [s]', 'Texposure_pos3 [s]',\n",
    "                                            'Texposure_pos4 [s]', 'zHop [mm]', 'layer height [mm]', 'texp.layer 2 [s]', 'texp.layer 3 [s]', \n",
    "                                            'texp.layer 4 [s]', 'texp.layer [s]', 'pictures'], axis=1)\n",
    "\n",
    "# Map texposure_pos1 [s]\n",
    "printingLogDf['texposure1_s'] = printingLogDf.apply(\n",
    "    lambda row: row['texposure_pos1 [s]'] if pd.notnull(row['texposure_pos1 [s]'])\n",
    "    else (row['Texposure_pos1 [s]'] if pd.notnull(row['Texposure_pos1 [s]'])\n",
    "    else row['texp.layer 1 [s]']), axis=1)\n",
    "\n",
    "printingLogDf = printingLogDf.drop(['texposure_pos1 [s]', 'Texposure_pos1 [s]', 'texp.layer 1 [s]'], axis=1)\n",
    "# ink <-\n",
    "# layerHeight <- 'layer height [µm]', layer height [mm]\n",
    "\n",
    "# Save to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + \"printingLog\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean rows (B4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe currently contains some cells, which can be filled from existing content. As mentioned, the column 'date' can be filled with values from 'foldername'. Furthermore 'foldername' can be used to enrich missing values in the 'title' column. Lastly, empty rows can be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the function to handle additional rules\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date(log):\n",
    "    # Extract the numeric part of the log string\n",
    "    numeric_part = re.search(r'\\d+', log)\n",
    "    if numeric_part:\n",
    "        numeric_part = numeric_part.group()\n",
    "        # If the numeric part ends with '01', '02', '03', '04', '05', '06', '07', which might not be part of the date, remove it\n",
    "        if numeric_part.endswith(('01', '02', '03', '04', '05', '06', '07')) and len(numeric_part) > 6:\n",
    "            numeric_part = numeric_part[:-2]\n",
    "        # Handle different date formats\n",
    "        if len(numeric_part) == 6:\n",
    "            # Assume the format is YYMMDD\n",
    "            date_format = '%y%m%d'\n",
    "        elif len(numeric_part) == 8:\n",
    "            # Assume the format is YYYYMMDD\n",
    "            date_format = '%Y%m%d'\n",
    "        else:\n",
    "            return None  # Return None if the format is unrecognized\n",
    "        # Parse the date\n",
    "        try:\n",
    "            return datetime.strptime(numeric_part, date_format).date()\n",
    "        except ValueError:\n",
    "            return None  # Return None if the date parsing fails\n",
    "    return None  # Return None if no numeric part is found\n",
    "\n",
    "# Apply the adjusted date extraction function\n",
    "overviewDf['date'] = overviewDf['foldername'].apply(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_title(row):\n",
    "    if row['title'] == '':\n",
    "        row['title'] = row['foldername']\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "overviewDf = overviewDf.apply(enrich_title, axis=1)\n",
    "\n",
    "overviewDf = overviewDf.drop(columns=['foldername'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns except 'id' since every row has an entry in that row\n",
    "columns_to_check = overviewDf.columns.drop('id')\n",
    "\n",
    "# Remove rows where any of the specified columns have an empty string\n",
    "overviewDf = overviewDf[~overviewDf[columns_to_check].map(lambda x: x == '').all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame back to a CSV file\n",
    "overviewDf.to_csv('data-frames-cleaned/' + \"overview\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bioinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's remove empty lines. First, characters indicating an empty cell are being replaced with an empty string to help streamline the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty-indicating characters\n",
    "rmCharacters = ['-', '•', '/', '%']\n",
    "bioInksDf = bioInksDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Replace empty strings with NaN for consistency\n",
    "bioInksDf = bioInksDf.replace('', np.nan)\n",
    "\n",
    "# Select all columns except 'logId'\n",
    "columns_to_check = bioInksDf.columns.drop('logid')\n",
    "\n",
    "# Remove rows where any of the specified columns are NaN (or empty after previous operations)\n",
    "bioInksDf = bioInksDf.dropna(subset=columns_to_check, how='all')\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column 'polymer'. We want to map similar bioinks to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioInksMappings = mappings[\"bioInks\"]\n",
    "\n",
    "def map_polymer(polymer):\n",
    "    polymer = polymer.strip()  # Remove any leading/trailing whitespace\n",
    "    return find_key(bioInksMappings['polymer'], polymer)\n",
    "    \n",
    "def map_solvent(solvent):\n",
    "    solvent = solvent.strip()  # Remove any leading/trailing whitespace\n",
    "    return find_key(bioInksMappings['solvent'], solvent)\n",
    "\n",
    "def removeChars(row):\n",
    "    polymerString = row['polymer']\n",
    "    solventString = row['solvent']\n",
    "\n",
    "    # Check if polymerString is a string, else set to empty string\n",
    "    if not isinstance(polymerString, str):\n",
    "        polymerString = ''\n",
    "    else:\n",
    "        polymerString = ''.join(filter(lambda x: not x.isdigit(), polymerString))\n",
    "    \n",
    "    splitStrings = polymerString.split() if polymerString else []\n",
    "\n",
    "    # Check if solventString is a string, else set to empty string\n",
    "    if not isinstance(solventString, str):\n",
    "        solventString = ''\n",
    "    \n",
    "    splitSolventStrings = solventString.split() if solventString else []\n",
    "    \n",
    "    # Map each potential ink and remove 'unknown' values\n",
    "    mapped_inks = [map_polymer(potentialInk) for potentialInk in splitStrings]\n",
    "    splitStrings = [ink for ink in mapped_inks if ink != 'other']\n",
    "    row['polymer'] = splitStrings\n",
    "    \n",
    "    # Map solvents\n",
    "    mapped_solvents = [map_solvent(potentialSolvent) for potentialSolvent in splitSolventStrings]\n",
    "    splitSolventStrings = [solvent for solvent in mapped_solvents if solvent != 'unknown']\n",
    "    row['solvent'] = splitSolventStrings\n",
    "             \n",
    "    return row\n",
    "    \n",
    "# column to lowercase\n",
    "bioInksDf['polymer'] = bioInksDf['polymer'].str.lower()\n",
    "bioInksDf['solvent'] = bioInksDf['solvent'].str.lower()\n",
    "\n",
    "bioInksDf = bioInksDf.apply(removeChars, axis=1)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to give every double ink in a row its own row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanForTwoInks(row, new_rows):\n",
    "    polymerArray = row['polymer']\n",
    "    # Check if cpolymer_v is a string, else set to empty string\n",
    "    cPolymers = row['cpolymer_v']\n",
    "    if not isinstance(cPolymers, str):\n",
    "        cPolymers = ''\n",
    "    else:\n",
    "        cPolymers = cPolymers.split()\n",
    "    solventsArray = row['solvent']\n",
    "    inkName = row['name']\n",
    "    if len(polymerArray) > 1 and len(cPolymers) > 1:\n",
    "        # Ensure that polymerArray and cPolymers have the same length\n",
    "        # This step is crucial as it ensures that there's a 1:1 mapping between polymers and concentrations\n",
    "        min_length = min(len(polymerArray), len(cPolymers))\n",
    "        polymerArray = polymerArray[:min_length]\n",
    "        cPolymers = cPolymers[:min_length]\n",
    "        \n",
    "        for i in range(min_length):\n",
    "            new_row = row.copy()\n",
    "            new_row['polymer'] = [polymerArray[i]]\n",
    "            new_row['cpolymer_v'] = [cPolymers[i]]\n",
    "            if len(solventsArray) == min_length:\n",
    "                new_row['solvents'] = solventsArray[i]\n",
    "            new_row['name'] = inkName\n",
    "            new_rows.append(new_row)\n",
    "            # Mark the index of the row to be dropped since it has been expanded into new rows\n",
    "            index_to_drop.append(row.name)\n",
    "    elif len(polymerArray) == 2 and len(cPolymers) == 1:\n",
    "        new_row = row.copy()\n",
    "        new_row['polymer'] = [polymerArray[0]]\n",
    "        new_row['cpolymer_v'] = [cPolymers[0]]\n",
    "        new_row['name'] = inkName\n",
    "        new_rows.append(new_row)\n",
    "        # Mark the index of the row to be dropped since it has been expanded into new rows\n",
    "        index_to_drop.append(row.name)\n",
    "    else:\n",
    "        new_rows.append(row)  # If there's only one polymer, keep the row as is\n",
    "        \n",
    "new_rows = []  # Initialize an empty list to collect new rows\n",
    "index_to_drop = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in bioInksDf.iterrows():\n",
    "    scanForTwoInks(row, new_rows)\n",
    "    \n",
    "# Drop the original rows that have been expanded into multiple rows\n",
    "bioInksDf = bioInksDf.drop(index_to_drop)\n",
    "    \n",
    "new_bioInksDf = pd.DataFrame(new_rows)\n",
    "\n",
    "bioInksDf = pd.concat([bioInksDf, new_bioInksDf], ignore_index=True)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the table some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    # polymer: Remove brackets and quotes\n",
    "    df['polymer'] = df['polymer'].str[0]\n",
    "\n",
    "    # cpolymer_v: Keep only numbers and replace commas with points\n",
    "    df['cpolymer_v'] = df['cpolymer_v'].str.replace(r'[^0-9,\\.]', '', regex=True)\n",
    "    df['cpolymer_v'] = df['cpolymer_v'].str.replace(',', '.')\n",
    "\n",
    "    # clap_wt: Remove non-numeric entries and entries with \">\"\n",
    "    df['clap_wt'] = df['clap_wt'].str.replace(r'[^0-9,\\.]', '', regex=True)\n",
    "    df['clap_wt'] = df['clap_wt'].str.replace(',', '.')\n",
    "    df['clap_wt'] = df['clap_wt'].replace(r'.*?>.*', '', regex=True)\n",
    "\n",
    "    # ctartrazine_mm: Same cleaning as clap_wt\n",
    "    df['ctartrazine_mm'] = df['ctartrazine_mm'].str.replace(r'[^0-9,\\.]', '', regex=True)\n",
    "    df['ctartrazine_mm'] = df['ctartrazine_mm'].str.replace(',', '.')\n",
    "    df['ctartrazine_mm'] = df['ctartrazine_mm'].replace(r'.*?>.*', '', regex=True)\n",
    "\n",
    "    # Solvent: Take the first entry and remove brackets and quotes\n",
    "    df['solvent'] = df['solvent'].str[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning function to the DataFrame\n",
    "bioInksDf = clean_dataframe(bioInksDf)\n",
    "\n",
    "bioInksDf = bioInksDf.drop(columns=['solvents'])\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fill missing numbers with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_zeros(row):\n",
    "    for column in ['cpolymer_v', 'clap_wt', 'ctartrazine_mm']:\n",
    "        if isinstance(row[column], str):\n",
    "            if row[column] == '':\n",
    "                row[column] = 0\n",
    "        try:\n",
    "            row[column] = float(row[column])\n",
    "        except ValueError:\n",
    "            row[column] = 0\n",
    "\n",
    "    return row\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "bioInksDf = bioInksDf.apply(fill_missing_with_zeros, axis=1)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicate rows which need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioInksDf = bioInksDf.drop_duplicates()\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + \"bioInks\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bioinks table looks clean now. ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, drop rows without content in the relevant columns 'printhead' and 'inkreservoir/drying'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific characters with empty strings\n",
    "rmCharacters = ['-', '•', '/', '%']\n",
    "hardwareSetupDf = hardwareSetupDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Select all columns except 'logId'\n",
    "columns_to_check = hardwareSetupDf.columns.drop(['logid', 'position'])\n",
    "\n",
    "# Remove rows where any of the specified columns have an empty string\n",
    "hardwareSetupDf = hardwareSetupDf[~hardwareSetupDf[columns_to_check].apply(lambda x: x == '').any(axis=1)]\n",
    "\n",
    "# Write to csv\n",
    "hardwareSetupDf.to_csv(\"data-frames-cleaned/\" + \"hardwareSetup\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardwareMappings = mappings['hardwareSetup']\n",
    "\n",
    "def map_values(row):\n",
    "    \n",
    "    # Handle NaN values and map printhead\n",
    "    printhead = str(row['printhead']) if pd.notna(row['printhead']) else ''\n",
    "    row['printhead'] = find_key(hardwareMappings['printhead'], printhead)\n",
    "\n",
    "    # Handle NaN values and map inkreservoir/drying\n",
    "    inkreservoir_drying = str(row['inkreservoir/drying']) if pd.notna(row['inkreservoir/drying']) else ''\n",
    "    row['inkreservoir/drying'] = find_key(hardwareMappings['inkreservoir_drying'], inkreservoir_drying)\n",
    "\n",
    "    return row\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "hardwareSetupDf = hardwareSetupDf.apply(map_values, axis=1)\n",
    "\n",
    "# Write to csv\n",
    "hardwareSetupDf.to_csv(\"data-frames-cleaned/\" + \"hardwareSetup\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardware table looks clean now. ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Settings Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific characters with empty strings\n",
    "rmCharacters = ['-', '•', '/', '%']\n",
    "materialSettingDf = materialSettingDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Select all columns except 'logId'\n",
    "columns_to_check = materialSettingDf.columns.drop(['logid', 'position'])\n",
    "\n",
    "# Remove rows where any of the specified columns have an empty string\n",
    "materialSettingDf = materialSettingDf[~materialSettingDf[columns_to_check].apply(lambda x: x == '').any(axis=1)]\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + \"materialSettings\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a few rows still miss values. Let's fill those with the mean value of the corresponding row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "columns_to_fill = ['temperature', 'temperaturetolerance', 'brightness', 'exposuretime', 'zhop', 'zhopspeed', 'washtime']\n",
    "\n",
    "materialSettingDf['exposuretime'] = materialSettingDf['exposuretime'].apply(lambda x: 0 if len(str(x)) > 3 else x)\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    materialSettingDf[column] = pd.to_numeric(materialSettingDf[column], errors='coerce')\n",
    "    \n",
    "\n",
    "column_means = materialSettingDf[columns_to_fill].mean()\n",
    "\n",
    "materialSettingDf.fillna(column_means, inplace=True)\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    materialSettingDf[column].replace(0, column_means[column], inplace=True)\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + \"materialSettings\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Log Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings with NaN\n",
    "# Remove rows where the 'status' column is NaN or empty\n",
    "printingLogDf = printingLogDf.dropna(subset=['status'])\n",
    "\n",
    "# If you also want to consider empty strings as missing values and remove those rows:\n",
    "printingLogDf = printingLogDf[printingLogDf['status'].astype(bool)]\n",
    "\n",
    "# Save to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + \"printingLog\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, map the 'status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "printingLogMappings = mappings[\"printingLog\"]\n",
    "\n",
    "def map_values(row):\n",
    "    # Map 'status' column\n",
    "    row['status'] = find_key(printingLogMappings['status'], row['status'])\n",
    "    return row\n",
    "    \n",
    "# column to lowercase\n",
    "printingLogDf['status'] = printingLogDf['status'].str.lower()\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "printingLogDf = printingLogDf.apply(map_values, axis=1)\n",
    "\n",
    "# Save to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + \"printingLog\" + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some annoying chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty-indicating characters\n",
    "rmCharacters = ['-', '•', '/']\n",
    "printingLogDf = printingLogDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Write to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + \"printingLog\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data Types (B5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the column \"Operator\". This column currently includes ambigous values. For example \"JV/TL\" and \"JV,TL\" mean the same thing.\n",
    "The following code cleans thoes ambiguities and puts the operators in an array of strings and thus converts it to a data type which can be processed more easily later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_operators(operator):\n",
    "    operator = operator.upper()\n",
    "    # Remove spaces around delimiters and parentheses\n",
    "    operator = re.sub(r'\\s*([,/&+()])\\s*', r'\\1', operator)\n",
    "    # Split on the delimiters\n",
    "    parts = re.split(r'[,/&+() ]', operator)\n",
    "    # Remove empty strings and strip whitespace\n",
    "    parts = [part.strip() for part in parts if part.strip()]\n",
    "    # Special handling for initials (e.g., 'John Doe' -> 'JD')\n",
    "    if len(parts) == 1 and ' ' in parts[0]:\n",
    "        parts = [''.join([name[0] for name in parts[0].split() if name])]\n",
    "        parts.upper()\n",
    "    return parts\n",
    "\n",
    "overviewDf[\"operators\"] = overviewDf[\"operator\"].apply(split_operators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good, now let's drop the \"operator\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewDf = overviewDf.drop(columns=['operator'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame back to a CSV file\n",
    "overviewDf.to_csv('data-frames-cleaned/' + \"overview\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overview table looks clean now. ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Storage (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a SQLite database from our cleaned dataframes. For that, we first need to split up the overview dataframe and create a new one for the 'operators' column.\n",
    "Also, we will merge materialSettings and hardwareSetup since both dataframes contain slot-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the operator_array to a new dataframe\n",
    "operator_data = [(row['id'], operator) for index, row in overviewDf.iterrows() for operator in row['operators']]\n",
    "operatorsDf = pd.DataFrame(operator_data, columns=['id', 'operator'])\n",
    "\n",
    "# Creating the main dataframe without the operator_array column\n",
    "overviewDf = overviewDf.drop('operators', axis=1)\n",
    "\n",
    "# Merge all slot related tables\n",
    "slotSettingsDf = pd.merge(materialSettingDf, hardwareSetupDf, on=['logid', 'position'])\n",
    "\n",
    "# save the changed dataframes again\n",
    "overviewDf.to_csv(\"data-frames-cleaned/\" + \"overview\" + '.csv', index=False)\n",
    "operatorsDf.to_csv(\"data-frames-cleaned/\" + \"operators\" + '.csv', index=False)\n",
    "slotSettingsDf.to_csv(\"data-frames-cleaned/\" + \"slotSettings\" + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "908"
      ]
     },
     "execution_count": 959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a new SQLite database\n",
    "conn = sqlite3.connect('../analysis/bioprinting.db')\n",
    "\n",
    "# create sqlite tables\n",
    "overviewDf.to_sql('overview', conn, if_exists='replace', index=False)\n",
    "operatorsDf.to_sql('operators', conn, if_exists='replace', index=False)\n",
    "slotSettingsDf.to_sql('slotSettings', conn, if_exists='replace', index=False)\n",
    "bioInksDf.to_sql('bioInks', conn, if_exists='replace', index=False)\n",
    "printingLogDf.to_sql('printingLog', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success percentage: 52.97356828193833%\n",
      "Failed percentage: 43.392070484581495%\n",
      "Other statuses percentage: 3.6343612334801763%\n"
     ]
    }
   ],
   "source": [
    "# SQL query to calculate the percentages\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  (SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS success_percentage,\n",
    "  (SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS failed_percentage,\n",
    "  (SUM(CASE WHEN status NOT IN ('success', 'failed') THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS other_percentage\n",
    "FROM\n",
    "  printingLog;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch the results\n",
    "percentages = cursor.fetchone()\n",
    "\n",
    "# Assign the percentages to variables\n",
    "success_percentage, failed_percentage, other_percentage = percentages\n",
    "\n",
    "# Print the results\n",
    "print(f\"Success percentage: {success_percentage}%\")\n",
    "print(f\"Failed percentage: {failed_percentage}%\")\n",
    "print(f\"Other statuses percentage: {other_percentage}%\")\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
