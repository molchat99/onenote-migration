{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the data cleaning phase of this project!\n",
    "We currently have a folder `table-data` with a bunch of dirty json files. The goal of this notebook is to illustrate the process of data cleaning by subsequently transforming the dirty json tables into a cohesive dataframe with tidied up data, ready to be analyzed!\n",
    "\n",
    "There are 6 different tables for every printing log:\n",
    "* Overview\n",
    "* Bioinks\n",
    "* Material Settings\n",
    "* Printer Setup\n",
    "* Hardware Setup\n",
    "* Printing Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's iterate over the json files and collect them in one large dataframe.\n",
    "\n",
    "Lets start with cleaning the \"overview\" table.\n",
    "\n",
    "The following code creates a dataframe, which is just a big table out of every json file that fits in the category \"overview\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"overview\"\n",
    "\n",
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, type + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "overviewDf = pd.DataFrame(data)\n",
    "\n",
    "overviewDf.to_csv(\"data-frames-raw/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's drop all the rows without content. Also, the column \"Log no.\" does not provide any additional information, so let's drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns except 'id'\n",
    "columns_to_check = overviewDf.columns.drop('id')\n",
    "\n",
    "# Remove rows where any of the specified columns have an empty string\n",
    "overviewDf = overviewDf[~overviewDf[columns_to_check].map(lambda x: x == '').all(axis=1)]\n",
    "\n",
    "overviewDf = overviewDf.drop(columns=['Log no.'])\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "overviewDf.to_csv('data-frames-cleaned/' + type + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is visible that the column \"Folder name\" contains valuable information, for example a date. But it is not normalized. Let's extract the date from that column and put it in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the function to handle additional rules\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date(log):\n",
    "    # Extract the numeric part of the log string\n",
    "    numeric_part = re.search(r'\\d+', log)\n",
    "    if numeric_part:\n",
    "        numeric_part = numeric_part.group()\n",
    "        # If the numeric part ends with '01', '02', '03', '04', '05', '06', '07', which might not be part of the date, remove it\n",
    "        if numeric_part.endswith(('01', '02', '03', '04', '05', '06', '07')) and len(numeric_part) > 6:\n",
    "            numeric_part = numeric_part[:-2]\n",
    "        # Handle different date formats\n",
    "        if len(numeric_part) == 6:\n",
    "            # Assume the format is YYMMDD\n",
    "            date_format = '%y%m%d'\n",
    "        elif len(numeric_part) == 8:\n",
    "            # Assume the format is YYYYMMDD\n",
    "            date_format = '%Y%m%d'\n",
    "        else:\n",
    "            return None  # Return None if the format is unrecognized\n",
    "        # Parse the date\n",
    "        try:\n",
    "            return datetime.strptime(numeric_part, date_format).date()\n",
    "        except ValueError:\n",
    "            return None  # Return None if the date parsing fails\n",
    "    return None  # Return None if no numeric part is found\n",
    "\n",
    "# Apply the adjusted date extraction function\n",
    "overviewDf['Date'] = overviewDf['folderName'].apply(extract_date)\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "overviewDf.to_csv('data-frames-cleaned/' + type + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked well. There are some rows which look like this: 281,Title,Operator,Description,Log230719-HAMA-30-2%-Thiol-ene\n",
    "This points to the table not being filled out by the operator. \n",
    "Let's enrich the \"Title\" row by taking the folderName as the title and replace the empty values with empty strings.\n",
    "After that, we can drop that column, since we extracted all useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_title(row):\n",
    "    # Check if the 'Title' column contains 'Title'\n",
    "    if row['Title'] == 'Title':\n",
    "       row['Title'] = row['folderName']\n",
    "       row['Operator'] = ''\n",
    "       row['Description'] = ''\n",
    "    elif row['Title'] == '':\n",
    "        row['Title'] = row['folderName']\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "overviewDf = overviewDf.apply(enrich_title, axis=1)\n",
    "\n",
    "overviewDf = overviewDf.drop(columns=['folderName'], axis=1)\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "overviewDf.to_csv('data-frames-cleaned/' + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the column \"Operator\". This column currently includes ambigous values. For example \"JV/TL\" and \"JV,TL\" mean the same thing.\n",
    "The following cell cleans thos ambiguities and puts the operators in an array of strings, which can be easily processed later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_operators(operator):\n",
    "    operator = operator.upper()\n",
    "    # Remove spaces around delimiters and parentheses\n",
    "    operator = re.sub(r'\\s*([,/&+()])\\s*', r'\\1', operator)\n",
    "    # Split on the delimiters\n",
    "    parts = re.split(r'[,/&+() ]', operator)\n",
    "    # Remove empty strings and strip whitespace\n",
    "    parts = [part.strip() for part in parts if part.strip()]\n",
    "    # Special handling for initials (e.g., 'John Doe' -> 'JD')\n",
    "    if len(parts) == 1 and ' ' in parts[0]:\n",
    "        parts = [''.join([name[0] for name in parts[0].split() if name])]\n",
    "        parts.upper()\n",
    "    return parts\n",
    "\n",
    "overviewDf[\"operator_array\"] = overviewDf[\"Operator\"].apply(split_operators)\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "overviewDf.to_csv('data-frames-cleaned/' + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good, now let's drop the \"operator\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewDf = overviewDf.drop(columns=['Operator'], axis=1)\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "overviewDf.to_csv('data-frames-cleaned/' + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overview table looks good now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the \"bioInks\" table next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"bioInks\"\n",
    "\n",
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, type + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "bioInksDf = pd.DataFrame(data)\n",
    "bioInksDf.to_csv(\"data-frames-raw/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has several columns which are mostly empty. Before removing columns, let's look at them, maybe we can map some to similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\n",
      "Polymer\n",
      "cpolymer [v%]\n",
      "cLAP [wt%]\n",
      "CTartrazine [mM]\n",
      "Solvent\n",
      "logId\n",
      "spheres\n",
      "CIodixanol [v/v]\n",
      "Gelma FIC A29\n",
      "other\n",
      "cLAP [v%]\n",
      "CDTT [v%]\n",
      "Color [mg]\n",
      "additives\n",
      "Additives\n",
      "Filter [µM]\n",
      "Add\n",
      "Fluorospheres [dil]\n",
      "Comment\n",
      "Amount of color(mg/ml)\n",
      "Beads Mio/ml\n",
      "cDTT [v%]\n",
      "Peptide (g/L)\n"
     ]
    }
   ],
   "source": [
    "for column in bioInksDf.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cLAP [v%] and cLAP [wt%] describe the same thing but are in different units. Let's change that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same goes for CDTT [v%] and CTartrazine [mM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioInksDf['CTartrazine [mM]'] = bioInksDf['CTartrazine [mM]'].combine_first(bioInksDf['CDTT [v%]'])\n",
    "# only get the values in mM\n",
    "\n",
    "def extract_mM(value):\n",
    "    # Check if value is a string\n",
    "    if isinstance(value, str):\n",
    "        # Use regular expression to find the pattern\n",
    "        match = re.search(r'\\((\\d+)mM\\)', value)\n",
    "        if match:\n",
    "            # Extract and return the number\n",
    "            return int(match.group(1))\n",
    "    # Return NaN or some default value if pattern not found or value is not a string\n",
    "    return value\n",
    "\n",
    "# Apply the function to the desired column\n",
    "bioInksDf['CTartrazine [mM]'] = bioInksDf['CTartrazine [mM]'].apply(extract_mM)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step we will remove the superfluous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns ['spheres', 'CIodixanol [v/v]', 'Gelma FIC A29', 'other', 'cLAP [v%]', 'CDTT [v%]', 'Color [mg]', 'additives', 'Additives', 'Filter [µM]', 'Add', 'Fluorospheres [dil]', 'Comment', 'Amount of color(mg/ml)', 'Beads Mio/ml', 'cDTT [v%]', 'Peptide (g/L)'] removed for type bioInks.\n"
     ]
    }
   ],
   "source": [
    "rmCols = [\n",
    "    \"spheres\" ,\n",
    "    \"CIodixanol [v/v]\",\n",
    "    \"Gelma FIC A29\",\n",
    "    \"other\",\n",
    "    \"cLAP [v%]\",\n",
    "    \"CDTT [v%]\",\n",
    "    \"Color [mg]\",\n",
    "    \"additives\",\n",
    "    \"Additives\",\n",
    "    \"Filter [µM]\",\n",
    "    \"Add\",\n",
    "    \"Fluorospheres [dil]\",\n",
    "    \"Comment\",\n",
    "    \"Amount of color(mg/ml)\",\n",
    "    \"Beads Mio/ml\",\n",
    "    \"cDTT [v%]\",\n",
    "    \"Peptide (g/L)\"\n",
    "]\n",
    "\n",
    "# Remove the specified columns from the DataFrame\n",
    "bioInksDf = bioInksDf.drop(columns=rmCols, errors='ignore')\n",
    "print(f\"Columns {rmCols} removed for type {type}.\")\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove empty lines. First, characters indicating an empty cell are being replaced with an empty string to help streamline the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty-indicating characters\n",
    "rmCharacters = ['-', '•', '/', '%']\n",
    "bioInksDf = bioInksDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Select all columns except 'logId'\n",
    "columns_to_check = bioInksDf.columns.drop('logId')\n",
    "\n",
    "# Remove rows where any of the specified columns have an empty string\n",
    "bioInksDf = bioInksDf[~bioInksDf[columns_to_check].map(lambda x: x == '').all(axis=1)]\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column 'polymer'. We want to map similar bioinks to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_polymer(polymer):\n",
    "    polymer = polymer.strip()  # Remove any leading/trailing whitespace\n",
    "    if 'gelma' in polymer or 'glema' in polymer:\n",
    "        return 'gelma'\n",
    "    elif 'hama' in polymer or ' hyaluronic' in polymer or 'hyaloron' in polymer or 'acid' in polymer:\n",
    "        return 'hama'\n",
    "    elif 'pegda' in polymer or 'pegda700' in polymer:\n",
    "        return 'pegda'\n",
    "    elif 'elma' in polymer:\n",
    "        return 'elma'\n",
    "    elif 'cmcma' in polymer or 'carboxymethyl' in polymer or 'chitosan' in polymer:\n",
    "        return 'cmcma'\n",
    "    elif 'dextran' in polymer:\n",
    "        return 'dextran'\n",
    "    elif 'handb' in polymer or 'ha_nb' in polymer or 'ha' in polymer and 'methacrylate' not in polymer and 'shares' not in polymer:\n",
    "        return 'hanb'\n",
    "    else:\n",
    "        return 'unknown'  # For polymers that don't match\n",
    "    \n",
    "def map_solvent(solvent):\n",
    "    solvent = solvent.strip()  # Remove any leading/trailing whitespace\n",
    "    if 'dpbs' in solvent:\n",
    "        return 'dpbs'\n",
    "    elif 'pbs' in solvent:\n",
    "        return 'pbs'\n",
    "    elif 'rpmi' in solvent:\n",
    "        return 'rpmi'\n",
    "    elif 'ddh2o' in solvent or 'ddh20' in solvent:\n",
    "        return 'ddh2o'\n",
    "    elif 'williams' in solvent:\n",
    "        return 'williams e'\n",
    "    else:\n",
    "        return 'unknown'  # For solvents that don't match\n",
    "\n",
    "def removeChars(row):\n",
    "    polymerString = row['Polymer']\n",
    "    solventString = row['Solvent']\n",
    "    polymerString = ''.join(filter(lambda x: not x.isdigit(), polymerString))\n",
    "    splitStrings = polymerString.split()\n",
    "    splitSolventStrings = solventString.split()\n",
    "    \n",
    "    # Map each potential ink and remove 'unknown' values\n",
    "    mapped_inks = [map_polymer(potentialInk) for potentialInk in splitStrings]\n",
    "    splitStrings = [ink for ink in mapped_inks if ink != 'unknown']\n",
    "    row['Polymer'] = splitStrings\n",
    "    \n",
    "    # Map solvents\n",
    "    mapped_solvents = [map_solvent(potentialSolvent) for potentialSolvent in splitSolventStrings]\n",
    "    splitSolventStrings = [solvent for solvent in mapped_solvents if solvent != 'unknown']\n",
    "    row['Solvent'] = splitSolventStrings\n",
    "             \n",
    "    return row\n",
    "    \n",
    "# column to lowercase\n",
    "bioInksDf['Polymer'] = bioInksDf['Polymer'].str.lower()\n",
    "bioInksDf['Solvent'] = bioInksDf['Solvent'].str.lower()\n",
    "\n",
    "bioInksDf = bioInksDf.apply(removeChars, axis=1)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to give every double ink in a row its own row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanForTwoInks(row, new_rows):\n",
    "    polymerArray = row['Polymer']\n",
    "    cPolymers = row['cpolymer [v%]'].split()\n",
    "    solventsArray = row['Solvent']\n",
    "    inkName = row['Name']\n",
    "    if len(polymerArray) > 1 and len(cPolymers) > 1:\n",
    "        # Ensure that polymerArray and cPolymers have the same length\n",
    "        # This step is crucial as it ensures that there's a 1:1 mapping between polymers and concentrations\n",
    "        min_length = min(len(polymerArray), len(cPolymers))\n",
    "        polymerArray = polymerArray[:min_length]\n",
    "        cPolymers = cPolymers[:min_length]\n",
    "        \n",
    "        for i in range(min_length):\n",
    "            new_row = row.copy()\n",
    "            new_row['Polymer'] = [polymerArray[i]]\n",
    "            new_row['cpolymer [v%]'] = [cPolymers[i]]\n",
    "            if len(solventsArray) == min_length:\n",
    "                new_row['Solvents'] = solventsArray[i]\n",
    "            new_row['Name'] = inkName\n",
    "            new_rows.append(new_row)\n",
    "            # Mark the index of the row to be dropped since it has been expanded into new rows\n",
    "            index_to_drop.append(row.name)\n",
    "    elif len(polymerArray) == 2 and len(cPolymers) == 1:\n",
    "        new_row = row.copy()\n",
    "        new_row['Polymer'] = [polymerArray[0]]\n",
    "        new_row['cpolymer [v%]'] = [cPolymers[0]]\n",
    "        new_row['Name'] = inkName\n",
    "        new_rows.append(new_row)\n",
    "        # Mark the index of the row to be dropped since it has been expanded into new rows\n",
    "        index_to_drop.append(row.name)\n",
    "    else:\n",
    "        new_rows.append(row)  # If there's only one polymer, keep the row as is\n",
    "        \n",
    "new_rows = []  # Initialize an empty list to collect new rows\n",
    "index_to_drop = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in bioInksDf.iterrows():\n",
    "    scanForTwoInks(row, new_rows)\n",
    "    \n",
    "# Drop the original rows that have been expanded into multiple rows\n",
    "bioInksDf = bioInksDf.drop(index_to_drop)\n",
    "    \n",
    "new_bioInksDf = pd.DataFrame(new_rows)\n",
    "\n",
    "bioInksDf = pd.concat([bioInksDf, new_bioInksDf], ignore_index=True)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the table some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    # Polymer: Remove brackets and quotes\n",
    "    df['Polymer'] = df['Polymer'].str[0]\n",
    "\n",
    "    # cpolymer [v%]: Keep only numbers and replace commas with points\n",
    "    df['cpolymer [v%]'] = df['cpolymer [v%]'].str.replace(r'[^0-9,\\.]', '', regex=True)\n",
    "    df['cpolymer [v%]'] = df['cpolymer [v%]'].str.replace(',', '.')\n",
    "\n",
    "    # cLAP [wt%]: Remove non-numeric entries and entries with \">\"\n",
    "    df['cLAP [wt%]'] = df['cLAP [wt%]'].str.replace(r'[^0-9,\\.]', '', regex=True)\n",
    "    df['cLAP [wt%]'] = df['cLAP [wt%]'].str.replace(',', '.')\n",
    "    df['cLAP [wt%]'] = df['cLAP [wt%]'].replace(r'.*?>.*', '', regex=True)\n",
    "\n",
    "    # CTartrazine [mM]: Same cleaning as cLAP\n",
    "    df['CTartrazine [mM]'] = df['CTartrazine [mM]'].str.replace(r'[^0-9,\\.]', '', regex=True)\n",
    "    df['CTartrazine [mM]'] = df['CTartrazine [mM]'].str.replace(',', '.')\n",
    "    df['CTartrazine [mM]'] = df['CTartrazine [mM]'].replace(r'.*?>.*', '', regex=True)\n",
    "\n",
    "    # Solvent: Take the first entry and remove brackets and quotes\n",
    "    df['Solvent'] = df['Solvent'].str[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning function to the DataFrame\n",
    "bioInksDf = clean_dataframe(bioInksDf)\n",
    "\n",
    "bioInksDf = bioInksDf.drop(columns=['Solvents'])\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fill missing numbers with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_zeros(row):\n",
    "    for column in ['cpolymer [v%]', 'cLAP [wt%]', 'CTartrazine [mM]']:\n",
    "        if isinstance(row[column], str):\n",
    "            if row[column] == '':\n",
    "                row[column] = 0\n",
    "        try:\n",
    "            row[column] = float(row[column])\n",
    "        except ValueError:\n",
    "            row[column] = 0\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "bioInksDf = bioInksDf.apply(fill_missing_with_zeros, axis=1)\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicate rows which need to be removed. Also, there are spaces in the column names. Replace them with a '_' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioInksDf = bioInksDf.drop_duplicates()\n",
    "bioInksDf.columns = bioInksDf.columns.str.replace(' ', '_').str.replace('[\\[\\]]', '', regex=True).str.replace('%', '')\n",
    "\n",
    "# Write to csv\n",
    "bioInksDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the hardware table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"hardwareSetup\"\n",
    "\n",
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, type + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "hardwareSetupDf = pd.DataFrame(data)\n",
    "\n",
    "hardwareSetupDf.to_csv(\"data-frames-raw/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove empty lines. First, characters indicating an empty cell are being replaced with an empty string to help streamline the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific characters with empty strings\n",
    "rmCharacters = ['-', '•', '/', '%']\n",
    "hardwareSetupDf = hardwareSetupDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Select all columns except 'logId'\n",
    "columns_to_check = hardwareSetupDf.columns.drop(['logId', 'Position'])\n",
    "\n",
    "# Remove rows where any of the specified columns have an empty string\n",
    "hardwareSetupDf = hardwareSetupDf[~hardwareSetupDf[columns_to_check].apply(lambda x: x == '').any(axis=1)]\n",
    "\n",
    "# Write to csv\n",
    "hardwareSetupDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's map \"Inkreservoir/Drying\", \"Ink reservoir/drying\" ,\"Ink reservoir/Drying\" to the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_concat = ['Inkreservoir/Drying', 'Ink reservoir/drying', 'Ink reservoir/Drying']\n",
    "\n",
    "hardwareSetupDf['inkreservoir/drying'] = hardwareSetupDf.apply(\n",
    "    lambda row: row['Inkreservoir/Drying'] if pd.notnull(row['Inkreservoir/Drying'])\n",
    "    else (row['Ink reservoir/drying'] if pd.notnull(row['Ink reservoir/drying'])\n",
    "    else row['Ink reservoir/Drying']), axis=1)\n",
    "\n",
    "hardwareSetupDf = hardwareSetupDf.drop(['Inkreservoir/Drying', 'Ink reservoir/drying', 'Ink reservoir/Drying', 'Printhead: CB5'], axis=1)\n",
    "\n",
    "# Write to csv\n",
    "hardwareSetupDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, map similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dish mappings\n",
    "\n",
    "# pol_001, black\n",
    "# al_001, black metal\n",
    "# al_002\n",
    "# al_003\n",
    "# al_004, hepabrick\n",
    "\n",
    "# al_007\n",
    "# al_008, hepabrick\n",
    "# al_009, blue\n",
    "# al_009, implant silver torpedo\n",
    "# al_010, blue\n",
    "# al_011, blue silver metal\n",
    "# al_012, tartrazino, silver\n",
    "# al_013, clear ink\n",
    "\n",
    "def map_values(row):\n",
    "    # map printhead col\n",
    "    if 'd4001' in row['Printhead'] or 'd40_01' in row['Printhead'] or 'br1' in row['Printhead']:\n",
    "        row['Printhead'] = 'd4001'\n",
    "    elif 'd4002' in row['Printhead'] or 'd40_02' in row['Printhead']:\n",
    "        row['Printhead'] = 'd4002'\n",
    "    elif 'd4003' in row['Printhead'] or 'd40-03' in row['Printhead'] or 'cr5' in row['Printhead']:\n",
    "        row['Printhead'] = 'd4003'\n",
    "    elif 'd4004' in row['Printhead'] or 'd40_04' in row['Printhead'] or 'gen4' in row['Printhead']:\n",
    "        row['Printhead'] = 'd4004'\n",
    "    elif 'small' in row['Printhead'] or 'Small' in row['Printhead']:\n",
    "        row['Printhead'] = 'small'\n",
    "    elif 'big' in row['Printhead'] or 'large' in row['Printhead'] or 'black metal' in row['Printhead'] or 'metal black' in row['Printhead'] or '4 cm' in row['Printhead'] or 'aluminium black' in row['Printhead'] or 'alu' in row['Printhead'] or 'black' in row['Printhead']:\n",
    "        row['Printhead'] = 'large'\n",
    "    else:\n",
    "        row['Printhead'] = 'other'\n",
    "    \n",
    "    # Define a pattern to match the codes, considering 'ai_' and 'al_' prefixes and optional underscores or spaces\n",
    "    pattern = r\"(ai_|al_)?_?(al_)?0?(\\d{2,3})\"\n",
    "    \n",
    "    # Search for the pattern in the 'inkreservoir/drying' column\n",
    "    match = re.search(pattern, row['inkreservoir/drying'])\n",
    "    if match:\n",
    "        # Construct the replacement string using the matched group which represents the number part\n",
    "        row['inkreservoir/drying'] = f\"al_{int(match.group(3)):03}\"\n",
    "    \n",
    "    if 'wash' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'wash'\n",
    "    elif 'silver metal' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'al_009'\n",
    "    elif 'al_060' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'al_006'\n",
    "    elif 'al_220' in row['inkreservoir/drying'] or 'al_200' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'al_002'\n",
    "    elif 'tartrazino' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'al_012'\n",
    "    elif 'blue metal' in row['inkreservoir/drying'] or 'aluminium blue' in row['inkreservoir/drying'] or 'metal blue' in row['inkreservoir/drying'] or 'aluminum blue' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'al_011'\n",
    "    elif 'big metal black' in row['inkreservoir/drying'] or 'black metal' in row['inkreservoir/drying'] or 'black alu vat' in row['inkreservoir/drying'] or 'metal black' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'al_001'\n",
    "    elif 'hepabrick' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'al_008'\n",
    "    elif 'dry' in row['inkreservoir/drying'] or 'drying' in row['inkreservoir/drying']:\n",
    "        row['inkreservoir/drying'] = 'dry'\n",
    "    else:\n",
    "        if not match:\n",
    "            row['inkreservoir/drying'] = 'other'\n",
    " \n",
    "    return row\n",
    "    \n",
    "# column to lowercase\n",
    "hardwareSetupDf['Printhead'] = hardwareSetupDf['Printhead'].str.lower()\n",
    "hardwareSetupDf['inkreservoir/drying'] = hardwareSetupDf['inkreservoir/drying'].str.lower()\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "hardwareSetupDf = hardwareSetupDf.apply(map_values, axis=1)\n",
    "\n",
    "# Write to csv\n",
    "hardwareSetupDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printer Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the printer setup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"printerSetup\"\n",
    "\n",
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, type + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "printerSetupDf = pd.DataFrame(data)\n",
    "\n",
    "printerSetupDf.to_csv(\"data-frames-raw/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printerSetup table only contains filenames and has a lot of missing values. Thus, it is considered irrelevant for this analysis and can be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the material settings table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"materialSettings\"\n",
    "\n",
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, type + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "materialSettingDf = pd.DataFrame(data)\n",
    "\n",
    "materialSettingDf.to_csv(\"data-frames-raw/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific characters with empty strings\n",
    "rmCharacters = ['-', '•', '/', '%']\n",
    "materialSettingDf = materialSettingDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Select all columns except 'logId'\n",
    "columns_to_check = materialSettingDf.columns.drop(['logId', 'Position'])\n",
    "\n",
    "# Remove rows where any of the specified columns have an empty string\n",
    "materialSettingDf = materialSettingDf[~materialSettingDf[columns_to_check].apply(lambda x: x == '').any(axis=1)]\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, merge the column .JSON with column \"Material parameters\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "materialSettingDf.loc[materialSettingDf['Material parameters'].isna(), 'Material parameters'] = materialSettingDf['.JSON']\n",
    "\n",
    "materialSettingDf = materialSettingDf.drop(['.JSON', 'Attempt', 'Material'], axis=1)\n",
    "\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"Material parameters\" has json values which need their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "columns_to_initialize = [\n",
    "    'temperature', 'temperatureTolerance', 'brightness', 'exposureTime', 'zHop',\n",
    "    'zHopSpeed', 'washTime', 'dabCount', 'washCount', 'projectionDelay'\n",
    "]\n",
    "for column in columns_to_initialize:\n",
    "    materialSettingDf[column] = \"\"\n",
    "\n",
    "def processJson(row):\n",
    "    string = row['Material parameters']\n",
    "    # Check for substrings that should cause the function to abort and return the string as is\n",
    "    if \"http\" in string or \"Slice\" in string:\n",
    "        return row\n",
    "    # Normalize the string by removing any leading/trailing braces and whitespace\n",
    "    if \"{\" in string or '\"' in string:   \n",
    "        normalized_str = re.sub(r'^\\s*{\\s*|\\s*}\\s*$', '', string)\n",
    "        pairs = re.findall(r'(?:\"(\\w+)\":\\s*([^,}]+))', normalized_str)\n",
    "    else:\n",
    "        normalized_str = string\n",
    "        pairs = re.findall(r'(?:(\\w+):\\s*([^,}]+))', normalized_str)\n",
    "\n",
    "    # Process each pair and convert numerical values from string to their appropriate type\n",
    "    for key, value in pairs:\n",
    "        # Remove any non-numeric trailing characters from the value\n",
    "        value = re.sub(r'[^0-9.]+$', '', value.strip())\n",
    "        # Try converting to integer, if fail, then to float, if fail, keep as string\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except ValueError:\n",
    "                pass  # keep the value as string if it's neither int nor float\n",
    "        # Assign the value to the row if the key is a column in the DataFrame\n",
    "        if key in materialSettingDf.columns:\n",
    "            row[key] = value\n",
    "            \n",
    "    return row\n",
    "\n",
    "materialSettingDf = materialSettingDf.apply(processJson, axis=1)\n",
    "\n",
    "# Drop useless columns\n",
    "materialSettingDf = materialSettingDf.drop([\"Material parameters\", \"File name\", \"File name of material .json\", \"File name of material .Json\"], axis=1)\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a few rows still miss values. Let's fill those with the mean value of the corresponding row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "columns_to_fill = ['temperature', 'temperatureTolerance', 'brightness', 'exposureTime', 'zHop', 'zHopSpeed', 'washTime']\n",
    "\n",
    "materialSettingDf['exposureTime'] = materialSettingDf['exposureTime'].apply(lambda x: 0 if len(str(x)) > 3 else x)\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    materialSettingDf[column] = pd.to_numeric(materialSettingDf[column], errors='coerce')\n",
    "    \n",
    "\n",
    "column_means = materialSettingDf[columns_to_fill].mean()\n",
    "\n",
    "materialSettingDf.fillna(column_means, inplace=True)\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    materialSettingDf[column].replace(0, column_means[column], inplace=True)\n",
    "\n",
    "# Write to csv\n",
    "materialSettingDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"printingLog\"\n",
    "\n",
    "data = []\n",
    "folder_path = 'table-data-cleaned'\n",
    "# Iterate through each subfolder\n",
    "for index, subfolder in enumerate(os.listdir(folder_path)):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check if \"bioinks.json\" exists in the subfolder\n",
    "        jsonPath = os.path.join(subfolder_path, type + \".json\")\n",
    "        if os.path.exists(jsonPath):\n",
    "            # Read the content of \"bioinks.json\" and append it to the list\n",
    "            with open(jsonPath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                data.extend(content)\n",
    "                \n",
    "# Convert the list to a pandas DataFrame\n",
    "printingLogDf = pd.DataFrame(data)\n",
    "\n",
    "printingLogDf.to_csv(\"data-frames-raw/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the columns and figuring out which columns describe the same values and which columns can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Attempt No.', 'Ink', 'layer height [µm]', 'no. bottom layers', 'texp.bottom\\xa0 [s]', 'texposure_pos1 [s]', 'texposure_pos2 [s]', 'texposure_pos3 [s]', 'texposure_pos4 [s]', 'zSpeed', 'status', 'comment', 'next steps', 'pictures', 'logId', 'texposure_sec[s]', 'texposure_last [s]', 'Washing/drying process', 'Ink1', 'Ink2', 'washing', 'drying', 'WASH/ DRY technique', 'GLOBAL OFFSET', 'texp.bottom\\u202f [s]', 'Texposure_pos1 [s]', 'Texposure_pos2 [s]', 'Texposure_pos3 [s]', 'Texposure_pos4 [s]', 'texp.layer [s]', 'zHop [mm]', 'LAP [%]', 'texp.layer 1 [s]', 'texp.layer 2 [s]', 'texp.layer 3 [s]', 'texp.layer 4 [s]', 'Post-curing time [min]', 'Tart', 'layer height [mm]', 'Base height', 'OFFSET', 'VARIABLE CHANGED', 'texposure [s]', 'Tart [mM]', 'zHop', 'Comment\\xa010 s nachbelichtet']\n"
     ]
    }
   ],
   "source": [
    "print(printingLogDf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "printingLogDf = printingLogDf.drop(columns=['OFFSET', 'Tart', 'Tart [mM]', 'Base height', 'zHop', 'Post-curing time [min]', 'VARIABLE CHANGED', \n",
    "                                            'Comment 10 s nachbelichtet', 'LAP [%]', 'GLOBAL OFFSET', 'texposure [s]', 'Ink1', 'Ink2', \n",
    "                                            'Washing/drying process', 'washing', 'drying', 'WASH/ DRY technique', 'texposure_sec[s]', \n",
    "                                            'texposure_last [s]', 'texp.bottom  [s]', 'Texposure_pos2 [s]', 'Texposure_pos3 [s]',\n",
    "                                            'Texposure_pos4 [s]', 'zHop [mm]', 'layer height [mm]', 'texp.layer 2 [s]', 'texp.layer 3 [s]', \n",
    "                                            'texp.layer 4 [s]', 'texp.layer [s]', 'pictures'], axis=1)\n",
    "\n",
    "# Map texposure_pos1 [s]\n",
    "printingLogDf['texposure1_s'] = printingLogDf.apply(\n",
    "    lambda row: row['texposure_pos1 [s]'] if pd.notnull(row['texposure_pos1 [s]'])\n",
    "    else (row['Texposure_pos1 [s]'] if pd.notnull(row['Texposure_pos1 [s]'])\n",
    "    else row['texp.layer 1 [s]']), axis=1)\n",
    "\n",
    "printingLogDf = printingLogDf.drop(['texposure_pos1 [s]', 'Texposure_pos1 [s]', 'texp.layer 1 [s]'], axis=1)\n",
    "# ink <-\n",
    "# layerHeight <- 'layer height [µm]', layer height [mm]\n",
    "\n",
    "# Save to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings with NaN\n",
    "# Remove rows where the 'status' column is NaN or empty\n",
    "printingLogDf = printingLogDf.dropna(subset=['status'])\n",
    "\n",
    "# If you also want to consider empty strings as missing values and remove those rows:\n",
    "printingLogDf = printingLogDf[printingLogDf['status'].astype(bool)]\n",
    "\n",
    "# Save to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, map the 'status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(row):\n",
    "    # map printhead col\n",
    "    statusRow = row['status']\n",
    "    \n",
    "    if 'success' in statusRow or 'Success' in statusRow or 'succes' in statusRow or 'succesfull' in statusRow or 'successfull' in statusRow or 'succsess' in statusRow or 'sucess' in statusRow or 'complete' in statusRow or 'ok' in statusRow or 'succe' in statusRow: \n",
    "        statusRow = 'success'\n",
    "    elif 'failed' in statusRow or 'fail' in statusRow:\n",
    "        statusRow = 'failed'\n",
    "    elif 'partial success' in statusRow or 'partial sucess' in statusRow or 'semi' in statusRow or 'almost good' in statusRow:\n",
    "        statusRow = 'partial success'\n",
    "    elif 'aborted' in statusRow or 'cancelled' in statusRow:\n",
    "        statusRow = 'aborted'\n",
    "    else:\n",
    "        statusRow = statusRow\n",
    "        \n",
    "    row['status'] = statusRow\n",
    " \n",
    "    return row\n",
    "    \n",
    "# column to lowercase\n",
    "printingLogDf['status'] = printingLogDf['status'].str.lower()\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "printingLogDf = printingLogDf.apply(map_values, axis=1)\n",
    "\n",
    "# Save to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some annoying chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty-indicating characters\n",
    "rmCharacters = ['-', '•', '/']\n",
    "printingLogDf = printingLogDf.replace(rmCharacters, '', regex=True)\n",
    "\n",
    "# Write to csv\n",
    "printingLogDf.to_csv(\"data-frames-cleaned/\" + type + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "908"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a new SQLite database\n",
    "conn = sqlite3.connect('../analysis/bioprinting.db')\n",
    "\n",
    "# Expanding the operator_array to a new dataframe\n",
    "operator_data = [(row['id'], operator) for index, row in overviewDf.iterrows() for operator in row['operator_array']]\n",
    "operatorsDf = pd.DataFrame(operator_data, columns=['id', 'operator'])\n",
    "\n",
    "# Creating the main dataframe without the operator_array column\n",
    "overviewDf = overviewDf.drop('operator_array', axis=1)\n",
    "\n",
    "# Merge all slot related tables\n",
    "slotSettingsDf = pd.merge(materialSettingDf, hardwareSetupDf, on=['logId', 'Position'])\n",
    "\n",
    "# save the changed dataframes again\n",
    "overviewDf.to_csv(\"data-frames-cleaned/\" + \"overview\" + '.csv', index=False)\n",
    "operatorsDf.to_csv(\"data-frames-cleaned/\" + \"operators\" + '.csv', index=False)\n",
    "slotSettingsDf.to_csv(\"data-frames-cleaned/\" + \"slotSettings\" + '.csv', index=False)\n",
    "\n",
    "# create sqlite tables\n",
    "overviewDf.to_sql('overview', conn, if_exists='replace', index=False)\n",
    "operatorsDf.to_sql('operators', conn, if_exists='replace', index=False)\n",
    "slotSettingsDf.to_sql('slotSettings', conn, if_exists='replace', index=False)\n",
    "bioInksDf.to_sql('bioInks', conn, if_exists='replace', index=False)\n",
    "printingLogDf.to_sql('printingLog', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success percentage: 52.97356828193833%\n",
      "Failed percentage: 43.392070484581495%\n",
      "Other statuses percentage: 3.6343612334801763%\n"
     ]
    }
   ],
   "source": [
    "# SQL query to calculate the percentages\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  (SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS success_percentage,\n",
    "  (SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS failed_percentage,\n",
    "  (SUM(CASE WHEN status NOT IN ('success', 'failed') THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS other_percentage\n",
    "FROM\n",
    "  printingLog;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch the results\n",
    "percentages = cursor.fetchone()\n",
    "\n",
    "# Assign the percentages to variables\n",
    "success_percentage, failed_percentage, other_percentage = percentages\n",
    "\n",
    "# Print the results\n",
    "print(f\"Success percentage: {success_percentage}%\")\n",
    "print(f\"Failed percentage: {failed_percentage}%\")\n",
    "print(f\"Other statuses percentage: {other_percentage}%\")\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
